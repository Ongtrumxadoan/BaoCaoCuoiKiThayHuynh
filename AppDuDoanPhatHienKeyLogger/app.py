import pandas as pd
import numpy as np
from collections import defaultdict
from flask import Flask, request, render_template, send_file  # S·ª≠ d·ª•ng render_template
import os
from werkzeug.utils import secure_filename
import joblib
import io
from elasticsearch import Elasticsearch, helpers
import csv
from flask import redirect, url_for
from flask import flash, get_flashed_messages


app = Flask(__name__)
app.secret_key = 'your_secret_key'  # Thay b·∫±ng chu·ªói b·∫•t k·ª≥

# Th∆∞ m·ª•c ƒë·ªÉ l∆∞u file t·∫£i l√™n v√† file k·∫øt qu·∫£
UPLOAD_FOLDER = 'uploads'
OUTPUT_FOLDER = 'outputs'
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['OUTPUT_FOLDER'] = OUTPUT_FOLDER


# K·∫øt n·ªëi Elasticsearch
es = Elasticsearch(
    "https://localhost:9200",
    verify_certs=False,
    basic_auth=("elastic", "CP=Gmq7Q8Rd0n02E-_W4")  # Thay b·∫±ng th√¥ng tin x√°c th·ª±c c·ªßa b·∫°n
)
# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(OUTPUT_FOLDER, exist_ok=True)


# Load m√¥ h√¨nh v√† scaler
scaler_path = r"model/scaler_71_features.pkl"
model_path = r"model/xgboost_best_model_71_features.pkl"

scaler = joblib.load(scaler_path)
model = joblib.load(model_path)

# Danh s√°ch c√°c c·ªôt ƒë·∫∑c tr∆∞ng c·∫ßn thi·∫øt (b·∫°n c·∫ßn thay b·∫±ng ƒë√∫ng 69 ƒë·∫∑c tr∆∞ng)
expected_features = scaler.feature_names_in_.tolist()  # n·∫øu b·∫°n d√πng sklearn >= 1.0

# H√†m t√≠nh to√°n 69 ƒë·∫∑c tr∆∞ng t·ª´ d·ªØ li·ªáu
def calculate_features(df):
    # Nh√≥m c√°c g√≥i tin th√†nh lu·ªìng (d·ª±a tr√™n 5-tuple: Src IP, Dst IP, Src Port, Dst Port, Protocol)
    flows = defaultdict(list)
    for idx, row in df.iterrows():
        flow_key = (row["Source"], row["Destination"], row["Source Port"], row["Destination Port"], row["Protocol"])
        flows[flow_key].append(row)

    # T√≠nh to√°n c√°c ƒë·∫∑c tr∆∞ng cho t·ª´ng lu·ªìng
    flow_features = []
    for flow_key, packets in flows.items():
        packets = pd.DataFrame(packets)
        
        # Flow Duration
        flow_duration = packets["Time"].max() - packets["Time"].min()
        
        # X√°c ƒë·ªãnh g√≥i tin Fwd v√† Bwd (d·ª±a tr√™n Source/Destination IP)
        src_ip = flow_key[0]
        fwd_packets = packets[packets["Source"] == src_ip]
        bwd_packets = packets[packets["Source"] != src_ip]
        
        # Total Fwd Packets v√† Total Backward Packets
        total_fwd_packets = len(fwd_packets)
        total_bwd_packets = len(bwd_packets)
        
        # Total Length of Fwd Packets v√† Total Length of Bwd Packets
        total_length_fwd = fwd_packets["Length"].sum()
        total_length_bwd = bwd_packets["Length"].sum()
        
        # Fwd Packet Length Mean v√† Bwd Packet Length Mean
        fwd_packet_len_mean = fwd_packets["Length"].mean() if total_fwd_packets > 0 else 0
        bwd_packet_len_mean = bwd_packets["Length"].mean() if total_bwd_packets > 0 else 0
        
        # Flow Bytes/s
        flow_bytes_s = (total_length_fwd + total_length_bwd) / flow_duration if flow_duration > 0 else 0
        
        # Flow Packets/s
        flow_packets_s = (total_fwd_packets + total_bwd_packets) / flow_duration if flow_duration > 0 else 0
        
        # Flow IAT Max, Flow IAT Min
        packet_times = packets["Time"].sort_values()
        iat = packet_times.diff().dropna()
        flow_iat_max = iat.max() if len(iat) > 0 else 0
        flow_iat_min = iat.min() if len(iat) > 0 else 0
        
        # Fwd IAT Total, Fwd IAT Mean, Fwd IAT Std, Fwd IAT Max, Fwd IAT Min
        fwd_times = fwd_packets["Time"].sort_values()
        fwd_iat = fwd_times.diff().dropna()
        fwd_iat_total = fwd_times.max() - fwd_times.min() if len(fwd_times) > 1 else 0
        fwd_iat_mean = fwd_iat.mean() if len(fwd_iat) > 0 else 0
        fwd_iat_std = fwd_iat.std() if len(fwd_iat) > 0 else 0
        fwd_iat_max = fwd_iat.max() if len(fwd_iat) > 0 else 0
        fwd_iat_min = fwd_iat.min() if len(fwd_iat) > 0 else 0
        
        # Bwd IAT Total, Bwd IAT Mean, Bwd IAT Std, Bwd IAT Max, Bwd IAT Min
        bwd_times = bwd_packets["Time"].sort_values()
        bwd_iat = bwd_times.diff().dropna()
        bwd_iat_total = bwd_times.max() - bwd_times.min() if len(bwd_times) > 1 else 0
        bwd_iat_mean = bwd_iat.mean() if len(bwd_iat) > 0 else 0
        bwd_iat_std = bwd_iat.std() if len(bwd_iat) > 0 else 0
        bwd_iat_max = bwd_iat.max() if len(bwd_iat) > 0 else 0
        bwd_iat_min = bwd_iat.min() if len(bwd_iat) > 0 else 0
        
        # ƒê·∫øm c√°c c·ªù (Flags)
        fwd_psh_flags = fwd_packets["Flags"].str.contains("PSH", na=False).sum()
        bwd_psh_flags = bwd_packets["Flags"].str.contains("PSH", na=False).sum()
        fwd_urg_flags = fwd_packets["Flags"].str.contains("URG", na=False).sum()
        bwd_urg_flags = bwd_packets["Flags"].str.contains("URG", na=False).sum()
        fin_flag_count = packets["Flags"].str.contains("FIN", na=False).sum()
        syn_flag_count = packets["Flags"].str.contains("SYN", na=False).sum()
        rst_flag_count = packets["Flags"].str.contains("RST", na=False).sum()
        psh_flag_count = packets["Flags"].str.contains("PSH", na=False).sum()
        ack_flag_count = packets["Flags"].str.contains("ACK", na=False).sum()
        urg_flag_count = packets["Flags"].str.contains("URG", na=False).sum()
        cwe_flag_count = packets["Flags"].str.contains("CWE", na=False).sum()
        ece_flag_count = packets["Flags"].str.contains("ECE", na=False).sum()
        
        # Fwd Header Length v√† Bwd Header Length (gi·∫£ s·ª≠ header length = 20 bytes cho TCP/IP)
        fwd_header_length = total_fwd_packets * 20
        bwd_header_length = total_bwd_packets * 20
        
        # Fwd Packets/s v√† Bwd Packets/s
        fwd_packets_s = total_fwd_packets / flow_duration if flow_duration > 0 else 0
        bwd_packets_s = total_bwd_packets / flow_duration if flow_duration > 0 else 0
        
        # Min Packet Length, Max Packet Length, Packet Length Mean, Packet Length Std, Packet Length Variance
        all_lengths = packets["Length"]
        min_packet_length = all_lengths.min() if len(all_lengths) > 0 else 0
        max_packet_length = all_lengths.max() if len(all_lengths) > 0 else 0
        packet_length_mean = all_lengths.mean() if len(all_lengths) > 0 else 0
        packet_length_std = all_lengths.std() if len(all_lengths) > 0 else 0
        packet_length_variance = all_lengths.var() if len(all_lengths) > 0 else 0
        
        # Down/Up Ratio
        down_up_ratio = total_bwd_packets / (total_fwd_packets + 1e-6)  # Tr√°nh chia cho 0
        
        # Average Packet Size
        average_packet_size = all_lengths.mean() if len(all_lengths) > 0 else 0
        
        # Avg Fwd Segment Size v√† Avg Bwd Segment Size
        avg_fwd_segment_size = fwd_packet_len_mean
        avg_bwd_segment_size = bwd_packet_len_mean
        
        # Fwd Header Length.1 (gi·∫£ s·ª≠ gi·ªëng Fwd Header Length)
        fwd_header_length_1 = fwd_header_length
        
        # Fwd Avg Bytes/Bulk, Fwd Avg Packets/Bulk, Fwd Avg Bulk Rate
        fwd_avg_bytes_bulk = 0
        fwd_avg_packets_bulk = 0
        fwd_avg_bulk_rate = 0
        
        # Bwd Avg Bytes/Bulk, Bwd Avg Packets/Bulk, Bwd Avg Bulk Rate
        bwd_avg_bytes_bulk = 0
        bwd_avg_packets_bulk = 0
        bwd_avg_bulk_rate = 0
        
        # Subflow Fwd Packets, Subflow Fwd Bytes, Subflow Bwd Packets, Subflow Bwd Bytes
        subflow_fwd_packets = total_fwd_packets
        subflow_fwd_bytes = total_length_fwd
        subflow_bwd_packets = total_bwd_packets
        subflow_bwd_bytes = total_length_bwd
        
        # Init_Win_bytes_forward v√† Init_Win_bytes_backward
        init_win_bytes_forward = -1
        init_win_bytes_backward = -1
        
        # act_data_pkt_fwd
        act_data_pkt_fwd = len(fwd_packets[fwd_packets["Length"] > 0])
        
        # min_seg_size_forward (gi·∫£ s·ª≠ b·∫±ng 20, gi·ªëng header length)
        min_seg_size_forward = 20
        
        # Active Mean, Active Std, Active Max, Active Min
        active_mean = 0
        active_std = 0
        active_max = 0
        active_min = 0
        
        # Idle Mean, Idle Std, Idle Max, Idle Min
        idle_mean = 0
        idle_std = 0
        idle_max = 0
        idle_min = 0
        
        # T·∫°o dictionary cho lu·ªìng n√†y v·ªõi ƒë√∫ng 69 ƒë·∫∑c tr∆∞ng
        flow_features.append({
            "Flow Duration": flow_duration,
            "Total Fwd Packets": total_fwd_packets,
            "Total Backward Packets": total_bwd_packets,
            "Total Length of Fwd Packets": total_length_fwd,
            "Total Length of Bwd Packets": total_length_bwd,
            "Fwd Packet Length Mean": fwd_packet_len_mean,
            "Bwd Packet Length Mean": bwd_packet_len_mean,
            "Flow Bytes/s": flow_bytes_s,
            "Flow Packets/s": flow_packets_s,
            "Flow IAT Max": flow_iat_max,
            "Flow IAT Min": flow_iat_min,
            "Fwd IAT Total": fwd_iat_total,
            "Fwd IAT Mean": fwd_iat_mean,
            "Fwd IAT Std": fwd_iat_std,
            "Fwd IAT Max": fwd_iat_max,
            "Fwd IAT Min": fwd_iat_min,
            "Bwd IAT Total": bwd_iat_total,
            "Bwd IAT Mean": bwd_iat_mean,
            "Bwd IAT Std": bwd_iat_std,
            "Bwd IAT Max": bwd_iat_max,
            "Bwd IAT Min": bwd_iat_min,
            "Fwd PSH Flags": fwd_psh_flags,
            "Bwd PSH Flags": bwd_psh_flags,
            "Fwd URG Flags": fwd_urg_flags,
            "Bwd URG Flags": bwd_urg_flags,
            "Fwd Header Length": fwd_header_length,
            "Bwd Header Length": bwd_header_length,
            "Fwd Packets/s": fwd_packets_s,
            "Bwd Packets/s": bwd_packets_s,
            "Min Packet Length": min_packet_length,
            "Max Packet Length": max_packet_length,
            "Packet Length Mean": packet_length_mean,
            "Packet Length Std": packet_length_std,
            "Packet Length Variance": packet_length_variance,
            "FIN Flag Count": fin_flag_count,
            "SYN Flag Count": syn_flag_count,
            "RST Flag Count": rst_flag_count,
            "PSH Flag Count": psh_flag_count,
            "ACK Flag Count": ack_flag_count,
            "URG Flag Count": urg_flag_count,
            "CWE Flag Count": cwe_flag_count,
            "ECE Flag Count": ece_flag_count,
            "Down/Up Ratio": down_up_ratio,
            "Average Packet Size": average_packet_size,
            "Avg Fwd Segment Size": avg_fwd_segment_size,
            "Avg Bwd Segment Size": avg_bwd_segment_size,
            "Fwd Header Length.1": fwd_header_length_1,
            "Fwd Avg Bytes/Bulk": fwd_avg_bytes_bulk,
            "Fwd Avg Packets/Bulk": fwd_avg_packets_bulk,
            "Fwd Avg Bulk Rate": fwd_avg_bulk_rate,
            "Bwd Avg Bytes/Bulk": bwd_avg_bytes_bulk,
            "Bwd Avg Packets/Bulk": bwd_avg_packets_bulk,
            "Bwd Avg Bulk Rate": bwd_avg_bulk_rate,
            "Subflow Fwd Packets": subflow_fwd_packets,
            "Subflow Fwd Bytes": subflow_fwd_bytes,
            "Subflow Bwd Packets": subflow_bwd_packets,
            "Subflow Bwd Bytes": subflow_bwd_bytes,
            "Init_Win_bytes_forward": init_win_bytes_forward,
            "Init_Win_bytes_backward": init_win_bytes_backward,
            "act_data_pkt_fwd": act_data_pkt_fwd,
            "min_seg_size_forward": min_seg_size_forward,
            "Active Mean": active_mean,
            "Active Std": active_std,
            "Active Max": active_max,
            "Active Min": active_min,
            "Idle Mean": idle_mean,
            "Idle Std": idle_std,
            "Idle Max": idle_max,
            "Idle Min": idle_min
        })

    return pd.DataFrame(flow_features)

# Route cho trang ch√≠nh
@app.route('/', methods=['GET', 'POST'])
def upload_file():
    if request.method == 'POST':
        # Ki·ªÉm tra xem c√≥ file ƒë∆∞·ª£c t·∫£i l√™n kh√¥ng
        if 'file' not in request.files:
            return render_template('index.html', error="Kh√¥ng t√¨m th·∫•y file. Vui l√≤ng ch·ªçn file ƒë·ªÉ t·∫£i l√™n.")
        
        file = request.files['file']
        
        # Ki·ªÉm tra xem ng∆∞·ªùi d√πng c√≥ ch·ªçn file kh√¥ng
        if file.filename == '':
            return render_template('index.html', error="Kh√¥ng c√≥ file n√†o ƒë∆∞·ª£c ch·ªçn.")
        
        if file and file.filename.endswith('.csv'):
            # L∆∞u file t·∫£i l√™n
            filename = secure_filename(file.filename)
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            file.save(filepath)
            
            try:
                # ƒê·ªçc file CSV
                df = pd.read_csv(filepath)
                
                # Ki·ªÉm tra c√°c c·ªôt c·∫ßn thi·∫øt
                required_columns = ["No.", "Time", "Source", "Destination", "Protocol", "Length", "Source Port", "Destination Port", "Flags", "Info"]
                missing_columns = [col for col in required_columns if col not in df.columns]
                if missing_columns:
                    return render_template('index.html', error=f"File CSV thi·∫øu c√°c c·ªôt: {', '.join(missing_columns)}")
                
                # T√≠nh to√°n ƒë·∫∑c tr∆∞ng
                df_flows = calculate_features(df)
                
                # L∆∞u k·∫øt qu·∫£ v√†o file CSV
                output_filename = f"flow_features_69_{filename}"
                output_filepath = os.path.join(app.config['OUTPUT_FOLDER'], output_filename)
                df_flows.to_csv(output_filepath, index=False)
                
                # Tr·∫£ v·ªÅ file ƒë·ªÉ t·∫£i xu·ªëng
                return send_file(
                    output_filepath,
                    as_attachment=True,
                    download_name=output_filename,
                    mimetype='text/csv'
                )
            
            except Exception as e:
                return render_template('index.html', error=f"C√≥ l·ªói x·∫£y ra: {str(e)}")
            
            finally:
                # X√≥a file t·∫£i l√™n sau khi x·ª≠ l√Ω
                if os.path.exists(filepath):
                    os.remove(filepath)
        
        else:
            return render_template('index.html', error="Vui l√≤ng t·∫£i l√™n file CSV.")
    
    # Hi·ªÉn th·ªã giao di·ªán n·∫øu l√† GET request
    return render_template('index.html')

# Route ƒë·ªÉ x√≥a file ƒë·∫ßu ra sau khi t·∫£i xu·ªëng
@app.route('/delete/<filename>')
def delete_file(filename):
    filepath = os.path.join(app.config['OUTPUT_FOLDER'], filename)
    if os.path.exists(filepath):
        os.remove(filepath)
    return '', 204


# @app.route('/predict', methods=['POST'])
# def predict():
#     file = request.files['file']
#     if not file:
#         return "‚ö†Ô∏è Kh√¥ng c√≥ file ƒë∆∞·ª£c t·∫£i l√™n!"

#     df = pd.read_csv(file)

#     if list(df.columns) != expected_features:
#         return f"‚ö†Ô∏è C·ªôt kh√¥ng kh·ªõp!<br>File c·∫ßn c√≥ 69 ƒë·∫∑c tr∆∞ng nh∆∞ sau:<br>{expected_features}"

#     # Chu·∫©n h√≥a v√† d·ª± ƒëo√°n
#     X_new = scaler.transform(df)
#     predictions = model.predict(X_new)
#     probabilities = model.predict_proba(X_new)[:, 1]

#     # Chu·∫©n b·ªã k·∫øt qu·∫£
#     results = []
#     for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
#         predicted_class = "Keylogger" if pred == 1 else "Benign"
#         results.append({
#             'Flow': i + 1,
#             'Class': predicted_class,
#             'Probability': round(prob, 4)
#         })

#     result_df = pd.DataFrame(results)

#     output = io.StringIO()
#     result_df.to_csv(output, index=False)
#     output.seek(0)

#     # Tr·∫£ v·ªÅ file CSV ƒë·ªÉ ng∆∞·ªùi d√πng t·∫£i v·ªÅ
#     return send_file(output, as_attachment=True, download_name="prediction_results.csv", mimetype="text/csv")


# Route hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n
@app.route('/predict', methods=['GET', 'POST'])
def predict():
    if request.method == 'POST':
        # X·ª≠ l√Ω t·∫£i l√™n v√† d·ª± ƒëo√°n
        file = request.files['file']
        if not file:
            return "‚ö†Ô∏è Kh√¥ng c√≥ file ƒë∆∞·ª£c t·∫£i l√™n!"

        df = pd.read_csv(file)
        # Ki·ªÉm tra c·ªôt v√† d·ª± ƒëo√°n nh∆∞ b·∫°n ƒë√£ l√†m tr∆∞·ªõc
        if list(df.columns) != expected_features:
            return f"‚ö†Ô∏è C·ªôt kh√¥ng kh·ªõp!<br>File c·∫ßn c√≥ 69 ƒë·∫∑c tr∆∞ng nh∆∞ sau:<br>{expected_features}"

        # Chu·∫©n h√≥a v√† d·ª± ƒëo√°n
        X_new = scaler.transform(df)
        predictions = model.predict(X_new)
        probabilities = model.predict_proba(X_new)[:, 1]

        results = []
        for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
            predicted_class = "Keylogger" if pred == 1 else "Benign"
            results.append({
                'flow': i + 1,
                'class': predicted_class,
                'probability': round(prob, 4)
            })

        global prediction_results
        prediction_results = results

        return render_template('result.html', results=results)
    
    # X·ª≠ l√Ω khi l√† GET request, tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë√£ c√≥
    return render_template('result.html', results=prediction_results)


# Route ƒë·ªÉ t·∫£i l√™n file CSV v√† g·ª≠i ƒë·∫øn Elasticsearch
@app.route('/upload_csv', methods=['POST'])
def upload_csv():
    file = request.files['file']
    if not file:
        return "‚ö†Ô∏è Kh√¥ng c√≥ file ƒë∆∞·ª£c t·∫£i l√™n!"

    # ƒê·ªçc file CSV t·ª´ form
    file_content = file.read().decode('utf-8')
    reader = csv.DictReader(io.StringIO(file_content))

    print("üßæ C√°c c·ªôt trong CSV:", reader.fieldnames)

    actions = [
        {
            "_index": "network_logvs2",
            "_source": {
                "Flow": int(row["Flow"]),
                "Class": row["Class"],
                "Probability": float(row["Probability(Keylogger)"])
            }
        }
        for row in reader
    ]

    # G·ª≠i d·ªØ li·ªáu bulk l√™n Elasticsearch
    helpers.bulk(es, actions)
    print("‚úÖ ƒê√£ upload th√†nh c√¥ng d·ªØ li·ªáu v√†o Elasticsearch.")
    flash("‚úÖ T·∫£i l√™n v√† g·ª≠i d·ªØ li·ªáu ƒë·∫øn Elasticsearch th√†nh c√¥ng!", "success")

    return redirect(url_for('predict'))  # Sau khi upload xong, quay l·∫°i trang d·ª± ƒëo√°n


@app.route('/dowloadcsv')
def dowloadcsv():
    global prediction_results
    if not prediction_results:
        return "‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ t·∫£i!"

    # T·∫°o DataFrame t·ª´ prediction_results
    df = pd.DataFrame(prediction_results)

    # ƒê·ªïi t√™n c·ªôt cho kh·ªõp v·ªõi b·∫£ng HTML
    df = df.rename(columns={
        'flow': 'Flow',
        'class': 'Class',
        'probability': 'Probability(Keylogger)'
    })

    # Ghi ra CSV trong b·ªô nh·ªõ
    output = io.StringIO()
    df.to_csv(output, index=False)
    output.seek(0)

    return send_file(
        io.BytesIO(output.getvalue().encode('utf-8')),
        mimetype='text/csv',
        as_attachment=True,
        download_name='du_doan_keylogger.csv'
    )

if __name__ == '__main__':
    app.run(debug=True)